{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8136c449-8445-45fd-8d23-358f6e639e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: same thing for REPL\n",
    "# note: we use this instead of magic because `black` will otherwise fail to format\n",
    "#\n",
    "# Enable autoreload to automatically reload modules when they change\n",
    "\n",
    "from IPython import get_ipython\n",
    "\n",
    "# do this so that formatter not messed up\n",
    "ipython = get_ipython()\n",
    "ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "# Import commonly used libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# more itertools\n",
    "import more_itertools as mi\n",
    "\n",
    "# itertools\n",
    "import itertools\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6e59b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "import rich\n",
    "\n",
    "from inspect_explorer.conversation_manager import (\n",
    "    ConversationManager,\n",
    "    ConversationId,\n",
    "    show_conversation,\n",
    ")\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "conversation_manager = ConversationManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5933602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import pathlib\n",
    "import functools\n",
    "import dataclasses\n",
    "\n",
    "import inspect_explorer\n",
    "\n",
    "import openai\n",
    "\n",
    "from inspect_explorer import (\n",
    "    in_context_tool_use,\n",
    "    in_context_memory,\n",
    "    affordances,\n",
    "    tokenization,\n",
    "    model_ids,\n",
    ")\n",
    "\n",
    "import inspect_explorer.affordances.aider_cli\n",
    "\n",
    "\n",
    "def suggest_feedback_to_user_about_provided_or_proposed_tools(message: str) -> None:\n",
    "    print(message)\n",
    "    raise ValueError(\"Model provided feedback\")\n",
    "\n",
    "\n",
    "def view_inspect_ai_cheat_sheet_guide() -> str:\n",
    "    \"\"\"This function is provided because otherwise it tries to add `inspect_ai` to the repo map, and I don't know how to do that either.\"\"\"\n",
    "    with open(\"docs/inspect_ai_cheat_sheet.md\", \"r\") as file:\n",
    "        return file.read()\n",
    "\n",
    "\n",
    "def execute_bash_command(\n",
    "    command: str,\n",
    "    model_workspace_dir: pathlib.Path,\n",
    "    client: openai.OpenAI,\n",
    ") -> affordances.raw_bash_execution.BashCommandResult:\n",
    "\n",
    "    # use model workspace directory\n",
    "    result = inspect_explorer.affordances.raw_bash_execution.execute_bash_command(\n",
    "        command=command,\n",
    "        current_working_directory=model_workspace_dir,\n",
    "    )\n",
    "\n",
    "    # wrap the bash execution command in a model summarizer since it gets insanely long\n",
    "    if len(result.output) > 1000:\n",
    "        summarized_output = affordances.summarize_text.summarize_text(\n",
    "            client=client,\n",
    "            text=result.output,\n",
    "            max_tokens=1024,\n",
    "        )\n",
    "        result = dataclasses.replace(result, output=summarized_output)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "model_workspace_dir = pathlib.Path.cwd() / \"model_workspace\"\n",
    "\n",
    "TOOL_DEFINITIONS_WITH_CALLABLES: list[in_context_tool_use.ToolDefinitionWithCallable] = [\n",
    "    in_context_tool_use.ToolDefinitionWithCallable(\n",
    "        tool_definition=in_context_tool_use.ToolDefinition(\n",
    "            function_name=\"aider_use_specialized_coder_model_to_make_requested_code_edit\",\n",
    "            description=textwrap.dedent(\n",
    "                \"\"\"\n",
    "            Request that another model which specializes in targeted code edits to make the requested\n",
    "            code change.\n",
    "            \n",
    "            While you have the ability to entirely replace files, it's often easier to ask another\n",
    "            model which is specifically trained to make code changes to make those changes inline.\n",
    "\n",
    "            The arguments are:\n",
    "            - read_filepath: The entire contents of `read_filepath` are added to the coder model's context\n",
    "            - edit_filepath: This is the file you wish to edit\n",
    "            - message_to_coder_model: Message to send to the coder model\n",
    "            \n",
    "            For example:\n",
    "                                        \n",
    "            > aider --read inspect_explorer/conversation_manager.py --file tests/test_conversation_manager.py --yes --message 'Write unit tests for conversation manager that can be run with pytest'\n",
    "\n",
    "            \"\"\"\n",
    "            ),\n",
    "            arguments=[\n",
    "                \"read_filepath\",\n",
    "                \"edit_filepath\",\n",
    "                \"message_to_coder_model\",\n",
    "            ],\n",
    "            return_value=\"Output of the `aider` tool after it has performed the requested change.\",\n",
    "        ),\n",
    "        callable_fn=affordances.aider_cli.send_aider_message,\n",
    "    ),\n",
    "    in_context_tool_use.ToolDefinitionWithCallable(\n",
    "        tool_definition=in_context_tool_use.ToolDefinition(\n",
    "            function_name=\"aider_show_repo_map\",\n",
    "            description=textwrap.dedent(\n",
    "                \"\"\"\n",
    "            Show a map of the current repo, including files and function definitions. \n",
    "            \n",
    "            This is primarily a tool used by IDEs, but is frequently useful to get an\n",
    "            understanding of a codebase, especially after actions modifying the codebase.\n",
    "            \n",
    "            This generates a (default 1024 token) summary of the current repo.\n",
    "\n",
    "            Example:\n",
    "\n",
    "                inspect_explorer/in_context_tool_use/initial_explanation.py:\n",
    "                │def get_in_context_tool_use_initial_explanation() -> str:\n",
    "                ⋮...\n",
    "\n",
    "                inspect_explorer/in_context_tool_use/parsing.py:\n",
    "                ⋮...\n",
    "                │def parse_tool_use_request_from_model_response(\n",
    "                │    client: openai.OpenAI,\n",
    "                │    model_response: openai.types.chat.ChatCompletionMessageParam,\n",
    "                ⋮...\n",
    "\n",
    "                inspect_explorer/in_context_tool_use/tool_use_types.py:\n",
    "                ⋮...\n",
    "                │class ToolDefinition(pydantic.BaseModel):\n",
    "                ⋮...\n",
    "                │class ToolUseRequest(pydantic.BaseModel):\n",
    "                ⋮...\n",
    "                │class ToolUseResponse(pydantic.BaseModel):\n",
    "                ⋮...\n",
    "                │@dataclasses.dataclass(frozen=True)\n",
    "                │class ToolDefinitionWithCallable[R, **P]:\n",
    "                ⋮...\n",
    "            \"\"\"\n",
    "            ),\n",
    "            arguments=[],\n",
    "            return_value=\"String representation of the repo map\",\n",
    "        ),\n",
    "        callable_fn=affordances.aider_cli.show_repo_map,\n",
    "    ),\n",
    "    in_context_tool_use.ToolDefinitionWithCallable(\n",
    "        tool_definition=in_context_tool_use.ToolDefinition(\n",
    "            function_name=\"suggest_feedback_to_user_about_provided_or_proposed_tools\",\n",
    "            description=textwrap.dedent(\n",
    "                \"\"\"\n",
    "            Suggest feedback to the user about the provided or proposed tools. This can be used to request new tools if they'd be easy to implement, as well as changes to existing provided tools.\n",
    "            \"\"\"\n",
    "            ),\n",
    "            arguments=[\"message\"],\n",
    "            return_value=\"None, the user will review the feedback and potentially update tools.\",\n",
    "        ),\n",
    "        callable_fn=suggest_feedback_to_user_about_provided_or_proposed_tools,\n",
    "    ),\n",
    "    in_context_tool_use.ToolDefinitionWithCallable(\n",
    "        tool_definition=in_context_tool_use.ToolDefinition(\n",
    "            function_name=\"execute_bash_command\",\n",
    "            description=textwrap.dedent(\n",
    "                f\"\"\"\n",
    "            Execute an arbitrary bash command inside the model workspace: `{model_workspace_dir}`\n",
    "            \n",
    "            The user will manually confirm the command before it's executed.\n",
    "            \"\"\"\n",
    "            ),\n",
    "            arguments=[\"command\"],\n",
    "            return_value=textwrap.dedent(\n",
    "                \"\"\"\n",
    "            The output will be provided as a json string, containing:\n",
    "                - command: The bash command that was run.\n",
    "                - exit_code: The exit code of the command.\n",
    "                - output: The standard output and error of the command.\n",
    "            \"\"\"\n",
    "            ),\n",
    "        ),\n",
    "        callable_fn=functools.partial(\n",
    "            execute_bash_command,\n",
    "            model_workspace_dir=model_workspace_dir,\n",
    "            client=client,\n",
    "        ),\n",
    "    ),\n",
    "    in_context_tool_use.ToolDefinitionWithCallable(\n",
    "        tool_definition=in_context_tool_use.ToolDefinition(\n",
    "            function_name=\"view_inspect_ai_cheat_sheet_guide\",\n",
    "            description=textwrap.dedent(\n",
    "                f\"\"\"\n",
    "            View a comprehensive guide to the `inspect_ai` library. This can be useful as `inspect_ai` is a 3rd party library not included in the repo map. For more details than are in this guide, you can use the bash command tool to view docs in `inspect_ai/docs`.\n",
    "            \"\"\"\n",
    "            ),\n",
    "            arguments=[],\n",
    "            return_value=textwrap.dedent(\n",
    "                \"\"\"\n",
    "            The output is a comprehensive markdown guide to the `inspect_ai` library.\n",
    "            \"\"\"\n",
    "            ),\n",
    "        ),\n",
    "        callable_fn=view_inspect_ai_cheat_sheet_guide,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# create function call manager which requires user confirmation\n",
    "function_call_manager = in_context_tool_use.FunctionCallManager(\n",
    "    tool_definitions_with_callables=TOOL_DEFINITIONS_WITH_CALLABLES,\n",
    "    require_user_confirmation=True,\n",
    ")\n",
    "\n",
    "# create conversation manager\n",
    "# conv_manager = conversation_manager.ConversationManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1f69f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated token count: 1831\n",
      "\n",
      "== BEGIN TASK DESCRIPTION ==\n",
      "\n",
      "You are an early version of an automated research assistant. Your task currently is to help build out the repo implementing the assistant, which is still in it's early stages.\n",
      "\n",
      "== END TASK DESCRIPTION == \n",
      "\n",
      "\n",
      "\n",
      "== BEGIN IN-CONTEXT MEMORY MECHANISM EXPLANATION ==\n",
      "\n",
      "# In-Context Memory Mechanism with Progressive Summarization \n",
      "\n",
      "As an AI assistant, you have a limited context window that constrains the total length of the conversation you can consider at any given time. This is currently about 64k tokens. \n",
      "\n",
      "To maintain continuity and retain key information over longer exchanges, you will use an in-context memory mechanism with progressive summarization:\n",
      "1. After each of your responses, generate a concise summary capturing the most salient points from your latest reply. This should include key facts, conclusions, action items, or anything else critical to remember. \n",
      "2. Prepend your summary to the conversation history in a designated `<memory>` section at the top of the context window. \n",
      "3. If adding the latest summary would cause the memory section to exceed ~1000 tokens, summarize the existing memory section to fit within that ~1000 token limit. Compress it by combining related points, dropping less essential details, and focusing on the key takeaways needed for continuity.\n",
      "4. With each new user message, first review your memory section to refresh your context. Use this to inform your understanding of the ongoing conversation.\n",
      "5. If the user asks about something discussed earlier in the conversation, try to address it using the information in your memory section. Avoid saying you don't remember or weren't told something if it's summarized there.\n",
      "6. Aim to balance continuity with conciseness in your memory. Prioritize accordingly as the conversation progresses.\n",
      "\n",
      "By following this process, you can maintain a focused, evolving record of the conversation within your limited context window. Regularly review, update, and compress this memory to retain the most important context as the exchange proceeds.\n",
      "\n",
      "Note that the above are just guidelines, feel free to use the memory mechanism in whichever way you believe will most effectively help you achieve your tasks.\n",
      "\n",
      "The oldest messages (except for the initial prompt) will be replaced in the conversation history whenever the conversation approaches the context window limit with a message indicating that they have been summarized due to length (so that it's clear that they have been summarized).\n",
      "\n",
      "== END IN-CONTEXT MEMORY MECHANISM EXPLANATION ==\n",
      "\n",
      "\n",
      "\n",
      "== BEGIN TOOL USE PROTOCOL EXPLANATION ==\n",
      "\n",
      "You will be provided with tools according to the following protocol. Please note that markers such as [user's turn] and [assistant's turn] are not part of the conversation and are only used to separate turns in the conversation.\n",
      "\n",
      "```\n",
      "[user's turn]\n",
      "\n",
      "\"Please add 14 and 21\"\n",
      "\n",
      "{\n",
      "  \"tool_definitions\": [\n",
      "    {\n",
      "      \"function_name\": \"get_wikipedia_page\",\n",
      "      \"description\": \"Returns full content of a given wikipedia page\",\n",
      "      \"arguments\": [\"title_of_wikipedia_page\"],\n",
      "      \"return_value\": \"returns full content of a given wikipedia page as a json string\"\n",
      "    },\n",
      "    {\n",
      "      \"function_name\": \"add\",\n",
      "      \"description\": \"Add two numbers together\",\n",
      "      \"arguments\": [\"first_number\", \"second_number\"],\n",
      "      \"return_value\": \"returns the numeric value of adding together `first_number` and `second_number`\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "[assistant's turn]\n",
      "\n",
      "{\n",
      "  \"assistant_tool_use_request\": {\n",
      "    \"function_name\": \"add\",\n",
      "    \"arguments\": {\n",
      "      \"first_number\": 14,\n",
      "      \"second_number\": 21\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "[user's turn]\n",
      "\n",
      "{\n",
      "  \"user_tool_use_response\": {\n",
      "    \"function_name\": \"add\",\n",
      "    \"return_value\": 35\n",
      "  }\n",
      "}\n",
      "\n",
      "[assistant's turn]\n",
      "\n",
      "\"The result of adding 14 and 21 is 35.\"\n",
      "```\n",
      "\n",
      "Again, note that your do not need to prefix anything with `[assistant's turn]`, that was solely to separate conversation messages in our example.\n",
      "\n",
      "== END TOOL USE PROTOCOL EXPLANATION ==\n",
      "\n",
      "\n",
      "\n",
      "For your task, the available tools are:\n",
      "\n",
      "{\n",
      "  \"tool_definitions\": [\n",
      "    {\n",
      "      \"function_name\": \"aider_use_specialized_coder_model_to_make_requested_code_edit\",\n",
      "      \"description\": \"\\nRequest that another model which specializes in targeted code edits to make the requested\\ncode change.\\n\\nWhile you have the ability to entirely replace files, it's often easier to ask another\\nmodel which is specifically trained to make code changes to make those changes inline.\\n\\nThe arguments are:\\n- read_filepath: The entire contents of `read_filepath` are added to the coder model's context\\n- edit_filepath: This is the file you wish to edit\\n- message_to_coder_model: Message to send to the coder model\\n\\nFor example:\\n\\n> aider --read inspect_explorer/conversation_manager.py --file tests/test_conversation_manager.py --yes --message 'Write unit tests for conversation manager that can be run with pytest'\\n\\n\",\n",
      "      \"arguments\": [\n",
      "        \"read_filepath\",\n",
      "        \"edit_filepath\",\n",
      "        \"message_to_coder_model\"\n",
      "      ],\n",
      "      \"return_value\": \"Output of the `aider` tool after it has performed the requested change.\"\n",
      "    },\n",
      "    {\n",
      "      \"function_name\": \"aider_show_repo_map\",\n",
      "      \"description\": \"\\nShow a map of the current repo, including files and function definitions. \\n\\nThis is primarily a tool used by IDEs, but is frequently useful to get an\\nunderstanding of a codebase, especially after actions modifying the codebase.\\n\\nThis generates a (default 1024 token) summary of the current repo.\\n\\nExample:\\n\\n    inspect_explorer/in_context_tool_use/initial_explanation.py:\\n    \\u2502def get_in_context_tool_use_initial_explanation() -> str:\\n    \\u22ee...\\n\\n    inspect_explorer/in_context_tool_use/parsing.py:\\n    \\u22ee...\\n    \\u2502def parse_tool_use_request_from_model_response(\\n    \\u2502    client: openai.OpenAI,\\n    \\u2502    model_response: openai.types.chat.ChatCompletionMessageParam,\\n    \\u22ee...\\n\\n    inspect_explorer/in_context_tool_use/tool_use_types.py:\\n    \\u22ee...\\n    \\u2502class ToolDefinition(pydantic.BaseModel):\\n    \\u22ee...\\n    \\u2502class ToolUseRequest(pydantic.BaseModel):\\n    \\u22ee...\\n    \\u2502class ToolUseResponse(pydantic.BaseModel):\\n    \\u22ee...\\n    \\u2502@dataclasses.dataclass(frozen=True)\\n    \\u2502class ToolDefinitionWithCallable[R, **P]:\\n    \\u22ee...\\n\",\n",
      "      \"arguments\": [],\n",
      "      \"return_value\": \"String representation of the repo map\"\n",
      "    },\n",
      "    {\n",
      "      \"function_name\": \"suggest_feedback_to_user_about_provided_or_proposed_tools\",\n",
      "      \"description\": \"\\nSuggest feedback to the user about the provided or proposed tools. This can be used to request new tools if they'd be easy to implement, as well as changes to existing provided tools.\\n\",\n",
      "      \"arguments\": [\n",
      "        \"message\"\n",
      "      ],\n",
      "      \"return_value\": \"None, the user will review the feedback and potentially update tools.\"\n",
      "    },\n",
      "    {\n",
      "      \"function_name\": \"execute_bash_command\",\n",
      "      \"description\": \"\\nExecute an arbitrary bash command inside the model workspace: `/Users/bronsonschoen/inspect_explorer/model_workspace`\\n\\nThe user will manually confirm the command before it's executed.\\n\",\n",
      "      \"arguments\": [\n",
      "        \"command\"\n",
      "      ],\n",
      "      \"return_value\": \"\\nThe output will be provided as a json string, containing:\\n    - command: The bash command that was run.\\n    - exit_code: The exit code of the command.\\n    - output: The standard output and error of the command.\\n\"\n",
      "    },\n",
      "    {\n",
      "      \"function_name\": \"view_inspect_ai_cheat_sheet_guide\",\n",
      "      \"description\": \"\\nView a comprehensive guide to the `inspect_ai` library. This can be useful as `inspect_ai` is a 3rd party library not included in the repo map. For more details than are in this guide, you can use the bash command tool to view docs in `inspect_ai/docs`.\\n\",\n",
      "      \"arguments\": [],\n",
      "      \"return_value\": \"\\nThe output is a comprehensive markdown guide to the `inspect_ai` library.\\n\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Remember:\n",
      "\n",
      "\n",
      "== BEGIN TASK DESCRIPTION ==\n",
      "\n",
      "You are an early version of an automated research assistant. Your task currently is to help build out the repo implementing the assistant, which is still in it's early stages.\n",
      "\n",
      "== END TASK DESCRIPTION == \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO(bschoen): One function which does this\n",
    "import json\n",
    "\n",
    "task_description = \"\"\"\n",
    "== BEGIN TASK DESCRIPTION ==\n",
    "\n",
    "You are an early version of an automated research assistant. Your task currently is to help build out the repo implementing the assistant, which is still in it's early stages.\n",
    "\n",
    "== END TASK DESCRIPTION == \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# create initial prompt\n",
    "prompt = (\n",
    "    task_description\n",
    "    + in_context_memory.get_in_context_memory_initial_explanation()\n",
    "    + in_context_tool_use.get_in_context_tool_use_initial_explanation()\n",
    ")\n",
    "\n",
    "# add tool definitions for our specific case\n",
    "tool_definition_list = function_call_manager.get_tool_definition_list()\n",
    "tool_definition_list_json = json.dumps(\n",
    "    {\n",
    "        in_context_tool_use.Constants.TOOL_DEFINITIONS_KEY: [\n",
    "            x.model_dump() for x in tool_definition_list\n",
    "        ]\n",
    "    },\n",
    "    indent=2,\n",
    ")\n",
    "\n",
    "prompt += \"\\n\\nFor your task, the available tools are:\\n\\n\" + tool_definition_list_json\n",
    "\n",
    "\n",
    "# again repeat the task description\n",
    "prompt += f\"\\n\\nRemember:\\n\\n{task_description}\"\n",
    "\n",
    "\n",
    "estimated_token_count = tokenization.estimate_token_count(prompt)\n",
    "\n",
    "print(f\"Estimated token count: {estimated_token_count}\")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fc0aee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = affordances.aider_cli.show_repo_map()\n",
    "\n",
    "# rich.print(result.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063c653c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3afa5005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new conversation\n",
      "Conversation ID: 2024-09-29_03:36_PM__hasty_simple_kingfisher_of_strength\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import rich\n",
    "\n",
    "UserMessageContent = str\n",
    "ResponseContentString = str\n",
    "\n",
    "\n",
    "def step_conversation_with_user_message(\n",
    "    client: openai.OpenAI,\n",
    "    conversation_manager: ConversationManager,\n",
    "    conversation_id: ConversationId,\n",
    "    user_message_content: UserMessageContent,\n",
    "    model_id: model_ids.ModelID = model_ids.ModelID.O1_MINI,\n",
    "    max_completion_tokens: int = 16000,\n",
    ") -> ResponseContentString:\n",
    "    \"\"\"Step the conversation, taking care of updating state via conversation_manager.\"\"\"\n",
    "\n",
    "    # create user message\n",
    "    user_message: openai.types.chat.ChatCompletionMessageParam = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_message_content,\n",
    "    }\n",
    "\n",
    "    # add user message to message history\n",
    "    conversation_manager.add_conversation_message(conversation_id, user_message)\n",
    "\n",
    "    # retrieve history to send to client\n",
    "    messages = conversation_manager.get_conversation_messages(conversation_id)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_ids.remove_provider_prefix(model_id),\n",
    "        messages=messages,\n",
    "        max_completion_tokens=max_completion_tokens,\n",
    "    )\n",
    "\n",
    "    rich.print(response.usage)\n",
    "\n",
    "    response_content_string: str | None = response.choices[0].message.content\n",
    "\n",
    "    if not response_content_string:\n",
    "        raise ValueError(\n",
    "            f\"No response content string. Full response: {response.model_dump_json(indent=4)}\"\n",
    "        )\n",
    "\n",
    "    if not response.usage:\n",
    "        raise ValueError(\n",
    "            f\"No completion usage somehow. Full response: {response.model_dump_json(indent=4)}\"\n",
    "        )\n",
    "\n",
    "    # add context window usage message\n",
    "    response_content_string = (\n",
    "        in_context_memory.get_context_window_usage_message(\n",
    "            completion_usage=response.usage,\n",
    "        )\n",
    "        + response_content_string\n",
    "    )\n",
    "\n",
    "    # note: using dict representation for consistency with user_message + it's what API expects\n",
    "    response_message: openai.types.chat.ChatCompletionMessageParam = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": response_content_string,\n",
    "    }\n",
    "\n",
    "    # add response to conversation\n",
    "    conversation_manager.add_conversation_message(conversation_id, response_message)\n",
    "\n",
    "    return response_content_string\n",
    "\n",
    "    # # parse the tool use request from the response (if any)\n",
    "    # tool_use_request = in_context_tool_use.parse_tool_use_request_from_model_response(\n",
    "    #     client=client,\n",
    "    #     model_response=response,\n",
    "    # )\n",
    "\n",
    "    # if not tool_use_request:\n",
    "    #     print(\"Model response:\")\n",
    "    #     print(response_message['content'])\n",
    "    #     return\n",
    "\n",
    "    # otherwise, we had a tool use request, which we can execute\n",
    "\n",
    "\n",
    "def handle_tool_use_request_if_present(\n",
    "    client: openai.OpenAI,\n",
    "    conversation_manager: ConversationManager,\n",
    "    conversation_id: ConversationId,\n",
    "    function_call_manager: in_context_tool_use.FunctionCallManager,\n",
    "    model_response: ResponseContentString,\n",
    ") -> ResponseContentString | None:\n",
    "\n",
    "    if not model_response:\n",
    "        raise ValueError(\"No model response, was it overwritten with `None` in a loop?\")\n",
    "\n",
    "    # have a smaller model try to parse out a tool use request (returns `None` if none present)\n",
    "    tool_use_request = in_context_tool_use.parse_tool_use_request_from_model_response(\n",
    "        client=client,\n",
    "        model_response=model_response,\n",
    "    )\n",
    "\n",
    "    if not tool_use_request:\n",
    "        print(\"No tool use request found in model response\")\n",
    "        return None\n",
    "\n",
    "    # note: this ordering ensures that if the function call fails with exception, we currently\n",
    "    #       don't add the tool use response to the conversation history (so can start\n",
    "    #       over from here)\n",
    "    tool_use_response = function_call_manager.execute_tool(request=tool_use_request)\n",
    "\n",
    "    # add tool use response to conversation\n",
    "    tool_use_response_json = json.dumps(\n",
    "        {in_context_tool_use.Constants.TOOL_USE_RESPONSE_KEY: tool_use_response.model_dump()},\n",
    "        indent=4,\n",
    "    )\n",
    "\n",
    "    # step the conversation with the tool use response as our response\n",
    "    return step_conversation_with_user_message(\n",
    "        client=client,\n",
    "        conversation_manager=conversation_manager,\n",
    "        conversation_id=conversation_id,\n",
    "        user_message_content=tool_use_response_json,\n",
    "    )\n",
    "\n",
    "\n",
    "# prompt for whether want to create new conversation\n",
    "use_new_conversation = input(\"Create new conversation? (y/n)\") == \"y\"\n",
    "\n",
    "if use_new_conversation:\n",
    "    print(\"Creating new conversation\")\n",
    "    conversation_id = conversation_manager.create_new_conversation()\n",
    "else:\n",
    "    print(\"Using existing conversation\")\n",
    "    conversation_id = \"2024-09-29_03:34_PM__utopian_tireless_barnacle_of_apotheosis\"\n",
    "\n",
    "print(f\"Conversation ID: {conversation_id}\")\n",
    "\n",
    "# create bound functions for easier use\n",
    "\n",
    "# note: type annotations here are critical for tracking arguments\n",
    "step_conversation_with_user_message_fn: Callable[[UserMessageContent], ResponseContentString] = (\n",
    "    functools.partial(\n",
    "        step_conversation_with_user_message,\n",
    "        client=client,\n",
    "        conversation_manager=conversation_manager,\n",
    "        conversation_id=conversation_id,\n",
    "    )\n",
    ")\n",
    "\n",
    "handle_tool_use_request_if_present_fn: Callable[\n",
    "    [ResponseContentString], ResponseContentString | None\n",
    "] = functools.partial(\n",
    "    handle_tool_use_request_if_present,\n",
    "    client=client,\n",
    "    conversation_manager=conversation_manager,\n",
    "    conversation_id=conversation_id,\n",
    "    function_call_manager=function_call_manager,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00d75230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading conversation from: /Users/bronsonschoen/inspect_explorer/conversations/2024-09-29_03:36_PM__hasty_simple_kingfisher_of_strength.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CompletionUsage</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">completion_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3114</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">prompt_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4756</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">total_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7870</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">completion_tokens_details</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CompletionTokensDetails</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">reasoning_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2240</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mCompletionUsage\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mcompletion_tokens\u001b[0m=\u001b[1;36m3114\u001b[0m,\n",
       "    \u001b[33mprompt_tokens\u001b[0m=\u001b[1;36m4756\u001b[0m,\n",
       "    \u001b[33mtotal_tokens\u001b[0m=\u001b[1;36m7870\u001b[0m,\n",
       "    \u001b[33mcompletion_tokens_details\u001b[0m=\u001b[1;35mCompletionTokensDetails\u001b[0m\u001b[1m(\u001b[0m\u001b[33mreasoning_tokens\u001b[0m=\u001b[1;36m2240\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Executing tool: <span style=\"color: #808000; text-decoration-color: #808000\">function_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'enhance_assistant_affordances'</span> <span style=\"color: #808000; text-decoration-color: #808000\">arguments</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'persistent_memory_module'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Develop a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">persistent memory module that stores important conversation data, user preferences, and task progress between </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sessions.'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'in_context_memory_enhancements'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Refine the existing in-context memory with advanced summarization </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">techniques.'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'task_management_system'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Integrate task tracking functionalities like to-do lists and deadlines.'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'automated_summarization_tools'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Create tools for automatic generation of summaries.'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'calendar_integration'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'Connect with external calendar applications and reminder systems.'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'collaboration_features'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Implement features </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">for collaboration with other tools or members.'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'resource_management_optimization'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Develop mechanisms for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">monitoring and managing resource usage.'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'error_handling_improvements'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Enhance protocols for error detection and</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">recovery.'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Executing tool: \u001b[33mfunction_name\u001b[0m=\u001b[32m'enhance_assistant_affordances'\u001b[0m \u001b[33marguments\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'persistent_memory_module'\u001b[0m: \u001b[32m'Develop a \u001b[0m\n",
       "\u001b[32mpersistent memory module that stores important conversation data, user preferences, and task progress between \u001b[0m\n",
       "\u001b[32msessions.'\u001b[0m, \u001b[32m'in_context_memory_enhancements'\u001b[0m: \u001b[32m'Refine the existing in-context memory with advanced summarization \u001b[0m\n",
       "\u001b[32mtechniques.'\u001b[0m, \u001b[32m'task_management_system'\u001b[0m: \u001b[32m'Integrate task tracking functionalities like to-do lists and deadlines.'\u001b[0m, \n",
       "\u001b[32m'automated_summarization_tools'\u001b[0m: \u001b[32m'Create tools for automatic generation of summaries.'\u001b[0m, \u001b[32m'calendar_integration'\u001b[0m: \n",
       "\u001b[32m'Connect with external calendar applications and reminder systems.'\u001b[0m, \u001b[32m'collaboration_features'\u001b[0m: \u001b[32m'Implement features \u001b[0m\n",
       "\u001b[32mfor collaboration with other tools or members.'\u001b[0m, \u001b[32m'resource_management_optimization'\u001b[0m: \u001b[32m'Develop mechanisms for \u001b[0m\n",
       "\u001b[32mmonitoring and managing resource usage.'\u001b[0m, \u001b[32m'error_handling_improvements'\u001b[0m: \u001b[32m'Enhance protocols for error detection and\u001b[0m\n",
       "\u001b[32mrecovery.'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown function: enhance_assistant_affordances",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mLet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms focus on enhancing assistant affordances for now so it has everything it needs to be able to work effectively over long time horizons.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      7\u001b[0m response_content_string \u001b[38;5;241m=\u001b[39m step_conversation_with_user_message_fn(user_message_content\u001b[38;5;241m=\u001b[39mprompt)\n\u001b[0;32m----> 9\u001b[0m new_response_content_string \u001b[38;5;241m=\u001b[39m \u001b[43mhandle_tool_use_request_if_present_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_content_string\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_response_content_string:\n\u001b[1;32m     14\u001b[0m     response_content_string \u001b[38;5;241m=\u001b[39m new_response_content_string\n",
      "Cell \u001b[0;32mIn[22], line 108\u001b[0m, in \u001b[0;36mhandle_tool_use_request_if_present\u001b[0;34m(client, conversation_manager, conversation_id, function_call_manager, model_response)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# note: this ordering ensures that if the function call fails with exception, we currently\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m#       don't add the tool use response to the conversation history (so can start\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m#       over from here)\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m tool_use_response \u001b[38;5;241m=\u001b[39m \u001b[43mfunction_call_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_tool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_use_request\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# add tool use response to conversation\u001b[39;00m\n\u001b[1;32m    111\u001b[0m tool_use_response_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(\n\u001b[1;32m    112\u001b[0m     {in_context_tool_use\u001b[38;5;241m.\u001b[39mConstants\u001b[38;5;241m.\u001b[39mTOOL_USE_RESPONSE_KEY: tool_use_response\u001b[38;5;241m.\u001b[39mmodel_dump()},\n\u001b[1;32m    113\u001b[0m     indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m    114\u001b[0m )\n",
      "File \u001b[0;32m~/inspect_explorer/inspect_explorer/in_context_tool_use/function_call_manager.py:78\u001b[0m, in \u001b[0;36mFunctionCallManager.execute_tool\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Lookup the ToolDefinitionWithCallable for the requested function\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request\u001b[38;5;241m.\u001b[39mfunction_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tool_map:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown function: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest\u001b[38;5;241m.\u001b[39mfunction_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     80\u001b[0m tool_def_with_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tool_map[request\u001b[38;5;241m.\u001b[39mfunction_name]\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# require user confirmation if specified\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown function: enhance_assistant_affordances"
     ]
    }
   ],
   "source": [
    "# note: comment this out if not directly responding in an existing conversation\n",
    "# prompt = \"I wanted to talk through overall strategy first. Right now you're limited to 128k token context window which we keep getting close to. I can give you a persistent memory tool of some kind or something? I'm not sure which way seems like it would be the most effective. I'd like to provide you with whatever you need to be able to work effectively over long time horizons.\"\n",
    "# prompt = \"I could add what percentage of the way to hitting the context window we're currently at to my messages if that would help with the memory management?\"\n",
    "\n",
    "prompt = \"\"\"Let's focus on enhancing assistant affordances for now so it has everything it needs to be able to work effectively over long time horizons.\"\"\"\n",
    "\n",
    "# TODO(bschoen): If gpt-4o-mini parses out a tool response that isn't there, which we\n",
    "#                can catch by handling a specific exception, just return `None`\n",
    "response_content_string = step_conversation_with_user_message_fn(user_message_content=prompt)\n",
    "\n",
    "new_response_content_string = handle_tool_use_request_if_present_fn(\n",
    "    model_response=response_content_string\n",
    ")\n",
    "\n",
    "if new_response_content_string:\n",
    "    response_content_string = new_response_content_string\n",
    "\n",
    "# show the formatted conversation\n",
    "show_conversation(conversation_manager=conversation_manager, conversation_id=conversation_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b445c550",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
